---
title: "Chapter 4"
output: html_document
---
## Question 2:Load the Boston data from the MASS package. Explore the structure and the dimensions of the data and describe the dataset briefly

```{r}

library(MASS)
library (tidyverse)
library (corrplot)
library (plotly)
#Load dataset Boston from MASS
data("Boston")

#look at data Boston
head (Boston)

#look at the structure of dataset
str(Boston)
data(Boston)

```
Data summary: This is the dataset about housing Values in Suburbs of Boston
Description: The Boston data frame has 506 rows and 14 columns with explaination of variables below:

*crim*: per capita crime rate by town.

*zn*: proportion of residential land zoned for lots over 25,000 sq.ft.

*indus*: proportion of non-retail business acres per town.

*chas*: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

*nox*: nitrogen oxides concentration (parts per 10 million).

*rm*: average number of rooms per dwelling.

*age*: proportion of owner-occupied units built prior to 1940.

*dis*: weighted mean of distances to five Boston employment centres.

*rad*: index of accessibility to radial highways.

*tax*: full-value property-tax rate per \$10,000.

*ptratio*: pupil-teacher ratio by town.

*black*: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

*lstat*: lower status of the population (percent).

*medv*: median value of owner-occupied homes in \$1000s.


## Question 3: a graphical overview of the data and show summaries of the variables in the data

```{r , echo=FALSE}
#Plot matrix
fig1=pairs(Boston)
print(fig1)
#correlations plot
cor_matrix <- cor (Boston)
#summary the result 
summary (cor_matrix%>%round(digits=2))
# visualize the correlation matrix 
corrplot(cor_matrix, method="square", type ="upper", cl.pos= "b", tl.pos="d", tl.cex=0.6)
#see correlation matrix with number of correalation...
corrplot.mixed (cor_matrix, lower.col = "black", number.cex = .7)
```
In fig1 shows the distribution of variables and their relationship. For example:*chas* variables is the categorical variable

## Question 4: Standardizing the data

```{r }
#scale variables
boston_scaled <- scale (Boston)
summary (boston_scaled%>%round (digits = 2))
#to see class of the scaled object
class (boston_scaled)
#change boston_scaled from matrix into data frame
boston_scaled <-as.data.frame(boston_scaled)
summary(Boston$crim)
#creat a quantile vector of crim 
bins <- quantile(boston_scaled$crim)
#see quantile parts of bins
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, labels = c("low", "med_low","med_high", "high") , include.lowest = TRUE)
table (crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set 
test <- boston_scaled[-ind,]

```

All means of variables move to 0. Now the crime variable is set to 4 categories according to how high is the per capita crime rate by town.

## Question 5: Linear discriminant analysis

```{r}
# linear discriminant analysis with target variable crime
lda_crime <- lda(crime ~ ., data = train)
lda_crime

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime)
# plot lda with 2 discriminants
plot(lda_crime, dimen = 2, col = classes, pch = classes)
lda.arrows(lda_crime, myscale = 1)
# eigenvalues:  
lda_crime$svd
```
The purpose of linear discriminant analysis (LDA) is to find the linear combinations of the original variables (in Boston dataset) that gives the best possible separation between the groups. 

We separate the *crime* into 4 groups: "low", "med_low","med_high", "high".
so the number of groups G=4, and the number of variables is 13 (p=13). The maximum number of useful discriminant functions that can separate is the minimum of G-1 and p.
Thus, we can find in the result 3 useful discriminant functions (Proportion of trace: LD1, LD2, LD3) to separate.

Ratio of the between- and within-group standard deviations on the linear discriminant variables is higher -> model is more precious.
LD1 = 0.96 means this model can discriminate 96% difference between groups

In the above picture we can see the linear combination of the variables that separate the crime classes. 

## Question 6) Predicting classes with the LDA model on the test data

```{r}
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# predict classes with test data
lda.pred <- predict(lda_crime, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

In the above picture we can see how the model perdicts the Crime classes. 
The model is able to predict closely to real value in "high"" and "low"" rates.

## Question 7) Standardazing the dataset and running the k-means algorithm
Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results.

```{r}
data("Boston")
boston_scaled2 <- scale(Boston)
boston_scaled2 <- as.data.frame(boston_scaled)
dist_eu <- dist(Boston)
summary (dist_eu)
dist_man <- dist(Boston, method = 'manhattan')
summary (dist_man)
# k-means clustering
km <-kmeans(Boston, centers = 4)
# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster )
#K-means: determine the k
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

*According to the qplot above, the optimal number of clusters is around 2, because afther that the WCSS changes dramatically*

```{r} 
km <-kmeans(Boston, centers = 2)
#plot focus on some variables
pairs(Boston , col = km$cluster)
pairs(Boston[1:5] , col = km$cluster)
pairs(Boston[6:10] , col = km$cluster)
pairs(Boston[11:14] , col = km$cluster)
```


### Super-Bonus:

```{r} 
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda_crime$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda_crime$scaling
matrix_product <- as.data.frame(matrix_product)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
```







