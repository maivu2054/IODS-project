---
title: "Chapter 4"
output: html_document
---
## Question 2:Load the Boston data from the MASS package. Explore the structure and the dimensions of the data and describe the dataset briefly

```{r}

library(MASS)
library (tidyverse)
library (corrplot)
library (plotly)
#Load dataset Boston from MASS
data("Boston")

#look at data Boston
View(Boston)
head (Boston)

#look at the structure of dataset
str(Boston)
data(Boston)

```
Data summary: This is the dataset about housing Values in Suburbs of Boston
Description: The Boston data frame has 506 rows and 14 columns with explaination of variables below:

*crim*: per capita crime rate by town.

*zn*: proportion of residential land zoned for lots over 25,000 sq.ft.

*indus*: proportion of non-retail business acres per town.

*chas*: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

*nox*: nitrogen oxides concentration (parts per 10 million).

*rm*: average number of rooms per dwelling.

*age*: proportion of owner-occupied units built prior to 1940.

*dis*: weighted mean of distances to five Boston employment centres.

*rad*: index of accessibility to radial highways.

*tax*: full-value property-tax rate per \$10,000.

*ptratio*: pupil-teacher ratio by town.

*black*: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

*lstat*: lower status of the population (percent).

*medv*: median value of owner-occupied homes in \$1000s.


## Question 3: a graphical overview of the data and show summaries of the variables in the data

```{r , echo=FALSE}
#Plot matrix
pairs(Boston)

#correlations plot
cor_matrix <- cor (Boston)
cor_matrix%>%round(digits=2)

# visualize the correlation matrix 
corrplot(cor_matrix, method="square", type ="upper", cl.pos= "b", tl.pos="d", tl.cex=0.6)
corrplot.mixed (cor_matrix, lower.col = "black", number.cex = .7)
```


```{r }
#scale variables
boston_scaled <- scale (Boston)
summary (boston_scaled)
#to see class of the scaled object
class (boston_scaled)
#change boston_scaled from matrix into data frame
boston_scaled <-as.data.frame(boston_scaled)
summary(Boston$crim)
#creat a quantile vector of crim 
bins <- quantile(boston_scaled$crim)
#see quantile parts of bins
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, labels = c("low", "med_low","med_high", "high") , include.lowest = TRUE)
table (crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set 
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

## 4) Standardizing the data

Now the crime variable is set to 4 categories according to how high is the per capita crime rate by town.

## 5) Linear discriminant analysis

```{r}
lda.fit <- lda(crime ~ ., data = train)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

In the above picture we can see the linear combination of the variables that separate the crime classes. The LDA model os visualized as a biplot

## 6) Predicting classes

```{r}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

In the above picture we can see how the model perdicts the Crime classes. Atleast for high crime rates the models predicition seems quite good 26/27. For med_low and med_high it isn't as accurate

## 7) Standardazing the dataset and running the k-means algorithm
Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results.

```{r}
boston_scaled2 <- scale(Boston)
boston_scaled2 <- as.data.frame(boston_scaled)
dist_eu <- dist(Boston)
dist_man <- dist(Boston, method = 'manhattan')
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

*According to the qplot above, the optimal number of clusters is around 2, because afther that the WCSS changes dramatically*

```{r} 
km <-kmeans(Boston, centers = 2)
pairs(Boston , col = km$cluster)
pairs(Boston[1:5] , col = km$cluster)
pairs(Boston[6:10] , col = km$cluster)
pairs(Boston[11:14] , col = km$cluster)
```

In the above pictures the variables are shown in pairs with 2 cluster groups. From this we can see how the two groups differentiate per pair variables. For example you can see how the data is clustered between proportion of blacks by town (black) and number of rooms per dwelling.


```{r} 
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
#dim(lda.fit$scaling)
# matrix multiplication
#matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
#matrix_product <- as.data.frame(matrix_product)
#plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
```






